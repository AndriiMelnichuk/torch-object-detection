{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOx51YRvvfc"
      },
      "source": [
        "#Детекция с использованием SSD\n",
        "Задача: создать и запрограмировать модель для детекции изображений. Модель может быть не одна.\n",
        "\n",
        "План:\n",
        "1.     Библиотеки\n",
        "2.     Загрузка и предобработка данных\n",
        "3.     (Опционально) Можно создать свой датасет для тренировки.\n",
        "\n",
        "Модель:\n",
        "\n",
        "Вход - изображение\n",
        "\n",
        "Выход - классы (с уверенностью), ббоксы. Идут последовательно\n",
        "\n",
        "Процесс обучения:\n",
        "Функции ошибки для классификации подаем элементы у которых IoU >= 0.5 и максимальные на их класс. Остальным должен соответствовать пустота.\n",
        "Функции ошибки для ббоксов должны подаваться те кто подавались на классы.\n",
        "\n",
        "Требование к модели:\n",
        "* Модель должна сохранять лучшие веса.\n",
        "* Модель предсказывает не точное местоположение в пикселях, а относительное.\n",
        "* При обучении идет обработка одновременно нескольких изображений.\n",
        "* Функция ошибки будет встроена в модель.\n",
        "\n",
        "\n",
        "Фишки которые хочу добавить:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_L9FJ7SytsA"
      },
      "source": [
        "# Библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rTto3zrvvIq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms as T\n",
        "from torch.nn import functional as F\n",
        "import torch.autograd.profiler as profiler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAuZ8wOj2hwm",
        "outputId": "ac1ee8aa-040d-45cd-d100-332608be517d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "CLASS_COUNT = 21\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 100\n",
        "BBOX_COUNT = 6\n",
        "EPOCH_COUNT = 10\n",
        "NUM2CLASS_DICT = {0: 'aeroplane',\n",
        "    1: 'bicycle',\n",
        "    2: 'bird',\n",
        "    3: 'boat',\n",
        "    4: 'bottle',\n",
        "    5: 'bus',\n",
        "    6: 'car',\n",
        "    7: 'cat',\n",
        "    8: 'chair',\n",
        "    9: 'cow',\n",
        "    10: 'diningtable',\n",
        "    11: 'dog',\n",
        "    12: 'horse',\n",
        "    13: 'motorbike',\n",
        "    14: 'person',\n",
        "    15: 'pottedplant',\n",
        "    16: 'sheep',\n",
        "    17: 'sofa',\n",
        "    18: 'train',\n",
        "    19: 'tvmonitor',\n",
        "    20: 'background'}\n",
        "CLASS2NUM_DICT = dict([(x[1], x[0]) for x in NUM2CLASS_DICT.items()])\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlf7WVZcy-9a"
      },
      "source": [
        "# Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKQIbfQnFSPH"
      },
      "outputs": [],
      "source": [
        "class ScaleKnowler():\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        smax = 0.9\n",
        "        smin = 0.2\n",
        "        m = 6\n",
        "        k = torch.arange(m) + 1\n",
        "        self.sk = smin + (smax - smin) * (k - 1) / (m - 1)\n",
        "\n",
        "    def getSk(self, k):\n",
        "        \"\"\"Нумерация идет от 0 до 5\"\"\"\n",
        "        if k > 5:\n",
        "            return 1\n",
        "        return self.sk[k].item()\n",
        "\n",
        "class AbsBboxKnowler():\n",
        "    def __init__(self):\n",
        "        self.ar = torch.tensor([1,2,3,1/2,1/3])\n",
        "        self.scale = ScaleKnowler()\n",
        "    def getBboxCount(self):\n",
        "        return self.ar.shape[0] + 1\n",
        "    def getAbsBbox(self, k):\n",
        "        # TODO: еще не проверял\n",
        "        sk = self.scale.getSk(k)\n",
        "        skP = self.scale.getSk(k+1)\n",
        "        w = sk * torch.sqrt(self.ar)\n",
        "        h = sk / torch.sqrt(self.ar)\n",
        "        whP = torch.sqrt(torch.tensor([sk * skP]))\n",
        "        w, h = torch.cat([w, whP]), torch.cat([h, whP])\n",
        "        return w, h\n",
        "\n",
        "\n",
        "class SSDClassificator(nn.Module):\n",
        "    def __init__(self, inChannel, classCount=CLASS_COUNT):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Conv2d(inChannel, classCount,3,1,1)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class SSDBBoxPredictor(nn.Module):\n",
        "    def __init__(self, inChannel, classCount=CLASS_COUNT):\n",
        "        super().__init__()\n",
        "        # dx, dy, cx, cy, 6 bboxes\n",
        "        # Структура вывода:\n",
        "        # (d,c)(1), (d,c)(2), (d,c)(3), (d,c)(4) - class 0\n",
        "        # (d,c)(1), (d,c)(2), (d,c)(3), (d,c)(4) - class 1\n",
        "        self.layer = nn.Conv2d(inChannel, 6*4*classCount,3,1,1)\n",
        "        self.a = nn.Tanh()\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "        x = self.a(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9MxRGnXvr4j"
      },
      "outputs": [],
      "source": [
        "class SSD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 3x224x224 -> 512x28x28\n",
        "        self.backbone = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT).features[:23]\n",
        "\n",
        "        # 512x28x28 -> 1024x13x13\n",
        "        self.l2 = nn.Sequential(\n",
        "                nn.Conv2d(512,1024,3,2),\n",
        "                nn.ReLU()\n",
        "        )\n",
        "        # 512x13x13 -> 1024x13x13\n",
        "        self.l3 = nn.Sequential(\n",
        "                nn.Conv2d(1024,1024,1),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        # 1024x13x13 -> 512x6x6\n",
        "        self.l4 = nn.Sequential(\n",
        "                nn.Conv2d(1024,256,1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(256,512,3,2),\n",
        "                nn.ReLU()\n",
        "        )\n",
        "        # 512x6x6 -> 256x3x3\n",
        "        self.l5 = nn.Sequential(\n",
        "                nn.Conv2d(512,128,1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(128,256,3,2,1),\n",
        "                nn.ReLU()\n",
        "        )\n",
        "        # 256x3x3 -> 256x1x1\n",
        "        self.l6 = nn.Sequential(\n",
        "                nn.Conv2d(256,128,1,1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(128,256,3),\n",
        "                nn.ReLU()\n",
        "        )\n",
        "        self.detectors = nn.ModuleList([\n",
        "                SSDBBoxPredictor(512),\n",
        "                SSDBBoxPredictor(1024),\n",
        "                SSDBBoxPredictor(1024),\n",
        "                SSDBBoxPredictor(512),\n",
        "                SSDBBoxPredictor(256),\n",
        "                SSDBBoxPredictor(256)\n",
        "        ])\n",
        "        self.classifier = nn.ModuleList([\n",
        "                SSDClassificator(512),\n",
        "                SSDClassificator(1024),\n",
        "                SSDClassificator(1024),\n",
        "                SSDClassificator(512),\n",
        "                SSDClassificator(256),\n",
        "                SSDClassificator(256)\n",
        "        ])\n",
        "    def forward(self, x):\n",
        "        prList = []\n",
        "        x = self.backbone(x)\n",
        "        prList.append(x)\n",
        "        x = self.l2(x)\n",
        "        prList.append(x)\n",
        "        x = self.l3(x)\n",
        "        prList.append(x)\n",
        "        x = self.l4(x)\n",
        "        prList.append(x)\n",
        "        x = self.l5(x)\n",
        "        prList.append(x)\n",
        "        x = self.l6(x)\n",
        "        prList.append(x)\n",
        "\n",
        "        classPred, bboxPred = [self.classifier[i](prList[i]) for i in range(6)], [self.detectors[i](prList[i]) for i in range(6)]\n",
        "\n",
        "        return classPred, bboxPred\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGTaYCL6vclE"
      },
      "source": [
        "# Функции утилиты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHm1R9IAz4s2"
      },
      "outputs": [],
      "source": [
        "@ torch.no_grad\n",
        "def selectBBoxByClass(classList, bboxList):\n",
        "    \"\"\"\n",
        "    Функция выбират только те ббоксы, которые соответствуют предсказаному классу.\n",
        "    Форма тензора в остальном остается той же\n",
        "    \"\"\"\n",
        "    res = []\n",
        "\n",
        "    for classPr, bboxPr in zip(classList, bboxList):\n",
        "        bboxNum = classPr.argmax(1).unsqueeze(1) * BBOX_COUNT * 4\n",
        "        indices = torch.arange(BBOX_COUNT * 4).view(1, BBOX_COUNT * 4, 1, 1).expand(bboxPr.shape[0], BBOX_COUNT * 4, bboxPr.shape[-2], bboxPr.shape[-1]).to(device)\n",
        "        indices = bboxNum + indices\n",
        "        result = torch.gather(bboxPr, 1, indices)\n",
        "        res.append(result)\n",
        "    return res\n",
        "\n",
        "def classList2ClassTensor(classList):\n",
        "    res = []\n",
        "    for pred in classList:\n",
        "        r = pred.view(pred.shape[0],21,-1).permute(0,2,1)\n",
        "        res.append(r)\n",
        "    res = torch.cat(res, 1)\n",
        "    res = res.repeat_interleave(BBOX_COUNT,1)\n",
        "    return res\n",
        "\n",
        "def bboxList2BboxTensor(bboxList):\n",
        "    absBboxKnowler = AbsBboxKnowler()\n",
        "    bboxCount = absBboxKnowler.getBboxCount()\n",
        "    res = []\n",
        "    for i in range(len(bboxList)):\n",
        "        data = bboxList[i]\n",
        "        x = torch.cat([(torch.arange(data.shape[-1]) + 0.5).view(1,-1)] * data.shape[-1]) / data.shape[-1]\n",
        "        x = x.to(device)\n",
        "        y = x.T\n",
        "        for i in range(bboxCount):\n",
        "            w, h = absBboxKnowler.getAbsBbox(i)\n",
        "            w, h = w.to(device), h.to(device)\n",
        "            # dx, dy, cx, cy\n",
        "            data[:,::4] += x\n",
        "            data[:,1::4] +=y\n",
        "            for j in range(4):\n",
        "                data[:,int(2+4*j)] *= w[j]\n",
        "                data[:,int(3+4*j)] *= h[j]\n",
        "        res.append(data.view(data.shape[0], 24, -1).permute(0,2,1).reshape(data.shape[0], -1, 4))\n",
        "    return torch.cat(res,1)\n",
        "\n",
        "def convertModelResult(classPred, bboxPred):\n",
        "    \"\"\"\n",
        "    Функция принимает списки предсказаний модели, и преабразует в два 2/3 мерных тензора.\n",
        "    \"\"\"\n",
        "    # TODO: Проверенно, но не идеально\n",
        "    return classList2ClassTensor(classPred), bboxList2BboxTensor(selectBBoxByClass(classPred, bboxPred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhxHCfS8yisH"
      },
      "source": [
        "# Работа с данными"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6_eh8JV01Y9"
      },
      "outputs": [],
      "source": [
        "def taregetPreProcessing(x):\n",
        "    \"\"\"\n",
        "    В результате мы должны получать такие данные:\n",
        "    - кординаты ббоксов в формате cxcywh / IMG_SIZE\n",
        "    - метку класса\n",
        "\n",
        "    \"\"\"\n",
        "    l = x['annotation']['object']\n",
        "    # l = trainDataset[0][1]['annotation']['object']\n",
        "\n",
        "    cord = [[int(y) for y in x['bndbox'].values()] for x in l]\n",
        "    cord = torch.tensor(cord)\n",
        "    cord = torchvision.ops.box_convert(cord, 'xyxy', 'cxcywh') / IMG_SIZE\n",
        "\n",
        "    lbl = [CLASS2NUM_DICT[x['name']] for x in l]\n",
        "    lbl = torch.tensor(lbl).view(1,-1).T\n",
        "    return lbl, cord\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    cord = []\n",
        "    lbl = []\n",
        "    objCount = []\n",
        "\n",
        "    for element in batch:\n",
        "        imgs.append(element[0])\n",
        "        cord.append(element[1][1])\n",
        "        lbl.append(element[1][0])\n",
        "        objCount.append(element[1][0].shape[0])\n",
        "\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "    cord = torch.cat(cord)\n",
        "    lbl = torch.cat(lbl).view(-1)\n",
        "    objCount = torch.tensor(objCount)\n",
        "\n",
        "    return imgs, cord, lbl, objCount\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp6cV-oTyh3i"
      },
      "outputs": [],
      "source": [
        "imgTransforms = T.Compose([\n",
        "        T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        T.ToTensor()\n",
        "])\n",
        "trainDataset = torchvision.datasets.VOCDetection(\n",
        "        root='data',\n",
        "        image_set='train',\n",
        "        # download=True,\n",
        "        transform=imgTransforms,\n",
        "        target_transform=taregetPreProcessing\n",
        ")\n",
        "testDataset = torchvision.datasets.VOCDetection(\n",
        "        root='data',\n",
        "        image_set='trainval',\n",
        "        # download=True,\n",
        "        transform=imgTransforms,\n",
        "        target_transform=taregetPreProcessing\n",
        ")\n",
        "\n",
        "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
        "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NessUKQnwLO6"
      },
      "source": [
        "# Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qcu1PahpwJ--"
      },
      "outputs": [],
      "source": [
        "# Инициализируем модель\n",
        "model = SSD().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
        "\n",
        "\n",
        "# Костыли\n",
        "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "# torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "%env TORCH_USE_CUDA_DSA=1\n",
        "# %%export TORCH_USE_CUDA_DSA=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt1x8pDPQ5Cl",
        "outputId": "7aa50f8b-2120-46c5-c623-6a78bd04260d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n",
            "env: TORCH_USE_CUDA_DSA=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torch.load('model.pth', weight_only)"
      ],
      "metadata": {
        "id": "4uoGZYy5cjTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XeC5XEe3NGI0",
        "outputId": "93658535-015d-4b04-9036-d1bb6264e591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "______________________\n",
            "Bathc: 1/58, loss: 707.025146484375\n",
            "Bathc: 2/58, loss: 649.7816772460938\n",
            "Bathc: 3/58, loss: 617.2135620117188\n",
            "Bathc: 4/58, loss: 587.9247436523438\n",
            "Bathc: 5/58, loss: 539.531494140625\n",
            "Bathc: 6/58, loss: 515.249755859375\n",
            "Bathc: 7/58, loss: 560.9061279296875\n",
            "Bathc: 8/58, loss: 598.77978515625\n",
            "Bathc: 9/58, loss: 714.049072265625\n",
            "Bathc: 10/58, loss: 596.368896484375\n",
            "Bathc: 11/58, loss: 532.8328247070312\n",
            "Bathc: 12/58, loss: 489.95654296875\n",
            "Bathc: 13/58, loss: 453.1678771972656\n",
            "Bathc: 14/58, loss: 463.7268371582031\n",
            "Bathc: 15/58, loss: 492.1435546875\n",
            "Bathc: 16/58, loss: 500.94635009765625\n",
            "Bathc: 17/58, loss: 523.8145141601562\n",
            "Bathc: 18/58, loss: 521.1709594726562\n",
            "Bathc: 19/58, loss: 494.1229248046875\n",
            "Bathc: 20/58, loss: 488.3055725097656\n",
            "Bathc: 21/58, loss: 454.9127502441406\n",
            "Bathc: 22/58, loss: 476.13006591796875\n",
            "Bathc: 23/58, loss: 541.3934936523438\n",
            "Bathc: 24/58, loss: 617.6058959960938\n",
            "Bathc: 25/58, loss: 721.648681640625\n",
            "Bathc: 26/58, loss: 689.8566284179688\n",
            "Bathc: 27/58, loss: 623.398193359375\n",
            "Bathc: 28/58, loss: 488.2298583984375\n",
            "Bathc: 29/58, loss: 553.4232788085938\n",
            "Bathc: 30/58, loss: 502.20068359375\n",
            "Bathc: 31/58, loss: 502.45172119140625\n",
            "Bathc: 32/58, loss: 490.2370300292969\n",
            "Bathc: 33/58, loss: 518.6651000976562\n",
            "Bathc: 34/58, loss: 506.3880920410156\n",
            "Bathc: 35/58, loss: 497.0867919921875\n",
            "Bathc: 36/58, loss: 513.14208984375\n",
            "Bathc: 37/58, loss: 507.8539123535156\n",
            "Bathc: 38/58, loss: 487.0860290527344\n",
            "Bathc: 39/58, loss: 1278.3900146484375\n",
            "Bathc: 40/58, loss: 587.4082641601562\n",
            "Bathc: 41/58, loss: 764.8743896484375\n",
            "Bathc: 42/58, loss: 652.5096435546875\n",
            "Bathc: 43/58, loss: 640.9612426757812\n",
            "Bathc: 44/58, loss: 591.7413940429688\n",
            "Bathc: 45/58, loss: 612.6798706054688\n",
            "Bathc: 46/58, loss: 561.79248046875\n",
            "Bathc: 47/58, loss: 577.4745483398438\n",
            "Bathc: 48/58, loss: 558.0436401367188\n",
            "Bathc: 49/58, loss: 562.1192016601562\n",
            "Bathc: 50/58, loss: 545.599365234375\n",
            "Bathc: 51/58, loss: 535.9415893554688\n",
            "Bathc: 52/58, loss: 535.9085083007812\n",
            "Bathc: 53/58, loss: 489.62298583984375\n",
            "Bathc: 54/58, loss: 492.0845642089844\n",
            "Bathc: 55/58, loss: 464.5669250488281\n",
            "Bathc: 56/58, loss: 513.3394165039062\n",
            "Bathc: 57/58, loss: 531.5252685546875\n",
            "Bathc: 58/58, loss: 100.81483459472656\n",
            "val loss: 637.3049926757812\n",
            "Epoch: 2\n",
            "______________________\n",
            "Bathc: 1/58, loss: 601.26318359375\n",
            "Bathc: 2/58, loss: 606.7037353515625\n",
            "Bathc: 3/58, loss: 713.9487915039062\n",
            "Bathc: 4/58, loss: 668.1113891601562\n",
            "Bathc: 5/58, loss: 587.6334228515625\n",
            "Bathc: 6/58, loss: 531.34814453125\n",
            "Bathc: 7/58, loss: 557.1471557617188\n",
            "Bathc: 8/58, loss: 478.5874938964844\n",
            "Bathc: 9/58, loss: 434.7408752441406\n",
            "Bathc: 10/58, loss: 453.8518981933594\n",
            "Bathc: 11/58, loss: 461.9051208496094\n",
            "Bathc: 12/58, loss: 491.2425842285156\n",
            "Bathc: 13/58, loss: 500.7650451660156\n",
            "Bathc: 14/58, loss: 541.0781860351562\n",
            "Bathc: 15/58, loss: 555.9046020507812\n",
            "Bathc: 16/58, loss: 537.8650512695312\n",
            "Bathc: 17/58, loss: 527.7357177734375\n",
            "Bathc: 18/58, loss: 550.318115234375\n",
            "Bathc: 19/58, loss: 536.9594116210938\n",
            "Bathc: 20/58, loss: 522.7353515625\n",
            "Bathc: 21/58, loss: 516.0515747070312\n",
            "Bathc: 22/58, loss: 490.73834228515625\n",
            "Bathc: 23/58, loss: 513.60986328125\n",
            "Bathc: 24/58, loss: 482.82110595703125\n",
            "Bathc: 25/58, loss: 469.4173583984375\n",
            "Bathc: 26/58, loss: 479.2412109375\n",
            "Bathc: 27/58, loss: 482.3419494628906\n",
            "Bathc: 28/58, loss: 480.9308776855469\n",
            "Bathc: 29/58, loss: 475.18609619140625\n",
            "Bathc: 30/58, loss: 465.9572448730469\n",
            "Bathc: 31/58, loss: 485.4880065917969\n",
            "Bathc: 32/58, loss: 467.93115234375\n",
            "Bathc: 33/58, loss: 474.5946960449219\n",
            "Bathc: 34/58, loss: 517.7931518554688\n",
            "Bathc: 35/58, loss: 472.68841552734375\n",
            "Bathc: 36/58, loss: 466.1783447265625\n",
            "Bathc: 37/58, loss: 469.9964599609375\n",
            "Bathc: 38/58, loss: 449.79473876953125\n",
            "Bathc: 39/58, loss: 457.77734375\n",
            "Bathc: 40/58, loss: 476.36865234375\n",
            "Bathc: 41/58, loss: 416.4538879394531\n",
            "Bathc: 42/58, loss: 414.76416015625\n",
            "Bathc: 43/58, loss: 452.15386962890625\n",
            "Bathc: 44/58, loss: 419.4420471191406\n",
            "Bathc: 45/58, loss: 477.8327331542969\n",
            "Bathc: 46/58, loss: 467.10076904296875\n",
            "Bathc: 47/58, loss: 453.06524658203125\n",
            "Bathc: 48/58, loss: 464.7516174316406\n",
            "Bathc: 49/58, loss: 465.75054931640625\n",
            "Bathc: 50/58, loss: 510.3454895019531\n",
            "Bathc: 51/58, loss: 469.0602111816406\n",
            "Bathc: 52/58, loss: 511.266845703125\n",
            "Bathc: 53/58, loss: 462.254638671875\n",
            "Bathc: 54/58, loss: 455.35369873046875\n",
            "Bathc: 55/58, loss: 443.9381408691406\n",
            "Bathc: 56/58, loss: 448.7250061035156\n",
            "Bathc: 57/58, loss: 446.64593505859375\n",
            "Bathc: 58/58, loss: 80.8634262084961\n",
            "val loss: 443.4572448730469\n",
            "Epoch: 3\n",
            "______________________\n",
            "Bathc: 1/58, loss: 443.5224914550781\n",
            "Bathc: 2/58, loss: 441.8341064453125\n",
            "Bathc: 3/58, loss: 437.0299987792969\n",
            "Bathc: 4/58, loss: 471.63531494140625\n",
            "Bathc: 5/58, loss: 478.82989501953125\n",
            "Bathc: 6/58, loss: 454.1123046875\n",
            "Bathc: 7/58, loss: 456.11407470703125\n",
            "Bathc: 8/58, loss: 424.8759765625\n",
            "Bathc: 9/58, loss: 426.262451171875\n",
            "Bathc: 10/58, loss: 429.66143798828125\n",
            "Bathc: 11/58, loss: 446.6994323730469\n",
            "Bathc: 12/58, loss: 427.17340087890625\n",
            "Bathc: 13/58, loss: 422.6337585449219\n",
            "Bathc: 14/58, loss: 435.6960754394531\n",
            "Bathc: 15/58, loss: 435.6331481933594\n",
            "Bathc: 16/58, loss: 417.15081787109375\n",
            "Bathc: 17/58, loss: 425.8597412109375\n",
            "Bathc: 18/58, loss: 406.9314880371094\n",
            "Bathc: 19/58, loss: 425.53057861328125\n",
            "Bathc: 20/58, loss: 464.1482238769531\n",
            "Bathc: 21/58, loss: 458.12969970703125\n",
            "Bathc: 22/58, loss: 465.73699951171875\n",
            "Bathc: 23/58, loss: 483.9844055175781\n",
            "Bathc: 24/58, loss: 453.2145690917969\n",
            "Bathc: 25/58, loss: 543.62939453125\n",
            "Bathc: 26/58, loss: 416.76055908203125\n",
            "Bathc: 27/58, loss: 413.6981506347656\n",
            "Bathc: 28/58, loss: 406.3221740722656\n",
            "Bathc: 29/58, loss: 390.3892822265625\n",
            "Bathc: 30/58, loss: 391.25604248046875\n",
            "Bathc: 31/58, loss: 399.4022216796875\n",
            "Bathc: 32/58, loss: 420.3082580566406\n",
            "Bathc: 33/58, loss: 431.9452819824219\n",
            "Bathc: 34/58, loss: 463.1924743652344\n",
            "Bathc: 35/58, loss: 507.9555358886719\n",
            "Bathc: 36/58, loss: 520.1050415039062\n",
            "Bathc: 37/58, loss: 520.942626953125\n",
            "Bathc: 38/58, loss: 536.6552734375\n",
            "Bathc: 39/58, loss: 556.41943359375\n",
            "Bathc: 40/58, loss: 561.2637329101562\n",
            "Bathc: 41/58, loss: 541.3202514648438\n",
            "Bathc: 42/58, loss: 528.0178833007812\n",
            "Bathc: 43/58, loss: 540.605712890625\n",
            "Bathc: 44/58, loss: 534.6498413085938\n",
            "Bathc: 45/58, loss: 518.47802734375\n",
            "Bathc: 46/58, loss: 519.2805786132812\n",
            "Bathc: 47/58, loss: 503.4665222167969\n",
            "Bathc: 48/58, loss: 514.579345703125\n",
            "Bathc: 49/58, loss: 512.5009155273438\n",
            "Bathc: 50/58, loss: 502.82843017578125\n",
            "Bathc: 51/58, loss: 495.3922119140625\n",
            "Bathc: 52/58, loss: 487.7552185058594\n",
            "Bathc: 53/58, loss: 470.667724609375\n",
            "Bathc: 54/58, loss: 476.1822204589844\n",
            "Bathc: 55/58, loss: 460.0727844238281\n",
            "Bathc: 56/58, loss: 476.3203125\n",
            "Bathc: 57/58, loss: 451.76385498046875\n",
            "Bathc: 58/58, loss: 81.21803283691406\n",
            "val loss: 468.8908386230469\n",
            "Epoch: 4\n",
            "______________________\n",
            "Bathc: 1/58, loss: 471.8138732910156\n",
            "Bathc: 2/58, loss: 480.11517333984375\n",
            "Bathc: 3/58, loss: 502.45599365234375\n",
            "Bathc: 4/58, loss: 498.1141662597656\n",
            "Bathc: 5/58, loss: 538.3674926757812\n",
            "Bathc: 6/58, loss: 518.7847900390625\n",
            "Bathc: 7/58, loss: 490.13165283203125\n",
            "Bathc: 8/58, loss: 541.1632690429688\n",
            "Bathc: 9/58, loss: 492.76898193359375\n",
            "Bathc: 10/58, loss: 506.43878173828125\n",
            "Bathc: 11/58, loss: 465.8902893066406\n",
            "Bathc: 12/58, loss: 454.0634765625\n",
            "Bathc: 13/58, loss: 420.4627990722656\n",
            "Bathc: 14/58, loss: 444.65673828125\n",
            "Bathc: 15/58, loss: 437.55078125\n",
            "Bathc: 16/58, loss: 484.11724853515625\n",
            "Bathc: 17/58, loss: 446.3421630859375\n",
            "Bathc: 18/58, loss: 458.2373352050781\n",
            "Bathc: 19/58, loss: 444.6666259765625\n",
            "Bathc: 20/58, loss: 483.6244812011719\n",
            "Bathc: 21/58, loss: 452.37847900390625\n",
            "Bathc: 22/58, loss: 491.7464904785156\n",
            "Bathc: 23/58, loss: 432.7611999511719\n",
            "Bathc: 24/58, loss: 411.4964294433594\n",
            "Bathc: 25/58, loss: 429.9673156738281\n",
            "Bathc: 26/58, loss: 430.3062744140625\n",
            "Bathc: 27/58, loss: 447.17120361328125\n",
            "Bathc: 28/58, loss: 437.4474182128906\n",
            "Bathc: 29/58, loss: 424.7550964355469\n",
            "Bathc: 30/58, loss: 452.1463317871094\n",
            "Bathc: 31/58, loss: 459.8849182128906\n",
            "Bathc: 32/58, loss: 438.1779479980469\n",
            "Bathc: 33/58, loss: 459.69476318359375\n",
            "Bathc: 34/58, loss: 439.9555358886719\n",
            "Bathc: 35/58, loss: 472.8204650878906\n",
            "Bathc: 36/58, loss: 462.14471435546875\n",
            "Bathc: 37/58, loss: 456.1539611816406\n",
            "Bathc: 38/58, loss: 457.54510498046875\n",
            "Bathc: 39/58, loss: 459.6758728027344\n",
            "Bathc: 40/58, loss: 455.63958740234375\n",
            "Bathc: 41/58, loss: 450.5315856933594\n",
            "Bathc: 42/58, loss: 446.81207275390625\n",
            "Bathc: 43/58, loss: 454.0763854980469\n",
            "Bathc: 44/58, loss: 446.72705078125\n",
            "Bathc: 45/58, loss: 432.7145690917969\n",
            "Bathc: 46/58, loss: 465.1150207519531\n",
            "Bathc: 47/58, loss: 435.6413269042969\n",
            "Bathc: 48/58, loss: 462.7810974121094\n",
            "Bathc: 49/58, loss: 449.7758483886719\n",
            "Bathc: 50/58, loss: 441.801025390625\n",
            "Bathc: 51/58, loss: 457.14569091796875\n",
            "Bathc: 52/58, loss: 448.5326232910156\n",
            "Bathc: 53/58, loss: 445.2283630371094\n",
            "Bathc: 54/58, loss: 466.6014404296875\n",
            "Bathc: 55/58, loss: 431.118408203125\n",
            "Bathc: 56/58, loss: 456.16754150390625\n",
            "Bathc: 57/58, loss: 454.7196350097656\n",
            "Bathc: 58/58, loss: 72.90602111816406\n",
            "val loss: 431.6571960449219\n",
            "Epoch: 5\n",
            "______________________\n",
            "Bathc: 1/58, loss: 424.4573059082031\n",
            "Bathc: 2/58, loss: 439.4928894042969\n",
            "Bathc: 3/58, loss: 440.2862854003906\n",
            "Bathc: 4/58, loss: 423.82196044921875\n",
            "Bathc: 5/58, loss: 426.938720703125\n",
            "Bathc: 6/58, loss: 428.6162109375\n",
            "Bathc: 7/58, loss: 452.98187255859375\n",
            "Bathc: 8/58, loss: 446.70721435546875\n",
            "Bathc: 9/58, loss: 438.901123046875\n",
            "Bathc: 10/58, loss: 422.54510498046875\n",
            "Bathc: 11/58, loss: 435.809814453125\n",
            "Bathc: 12/58, loss: 417.30975341796875\n",
            "Bathc: 13/58, loss: 415.0133361816406\n",
            "Bathc: 14/58, loss: 441.1485900878906\n",
            "Bathc: 15/58, loss: 442.6980285644531\n",
            "Bathc: 16/58, loss: 423.02862548828125\n",
            "Bathc: 17/58, loss: 443.9317626953125\n",
            "Bathc: 18/58, loss: 429.19989013671875\n",
            "Bathc: 19/58, loss: 450.98345947265625\n",
            "Bathc: 20/58, loss: 423.5938720703125\n",
            "Bathc: 21/58, loss: 454.83599853515625\n",
            "Bathc: 22/58, loss: 444.7405700683594\n",
            "Bathc: 23/58, loss: 438.0106201171875\n",
            "Bathc: 24/58, loss: 441.3932800292969\n",
            "Bathc: 25/58, loss: 458.535888671875\n",
            "Bathc: 26/58, loss: 447.1011657714844\n",
            "Bathc: 27/58, loss: 438.001708984375\n",
            "Bathc: 28/58, loss: 458.55908203125\n",
            "Bathc: 29/58, loss: 455.72509765625\n",
            "Bathc: 30/58, loss: 422.3699951171875\n",
            "Bathc: 31/58, loss: 451.16796875\n",
            "Bathc: 32/58, loss: 446.12237548828125\n",
            "Bathc: 33/58, loss: 454.72454833984375\n",
            "Bathc: 34/58, loss: 447.695556640625\n",
            "Bathc: 35/58, loss: 426.60443115234375\n",
            "Bathc: 36/58, loss: 419.9649963378906\n",
            "Bathc: 37/58, loss: 451.93267822265625\n",
            "Bathc: 38/58, loss: 443.19830322265625\n",
            "Bathc: 39/58, loss: 436.6830139160156\n",
            "Bathc: 40/58, loss: 471.3050537109375\n",
            "Bathc: 41/58, loss: 487.3841552734375\n",
            "Bathc: 42/58, loss: 494.38970947265625\n",
            "Bathc: 43/58, loss: 462.0360412597656\n",
            "Bathc: 44/58, loss: 463.86431884765625\n",
            "Bathc: 45/58, loss: 414.03057861328125\n",
            "Bathc: 46/58, loss: 402.143310546875\n",
            "Bathc: 47/58, loss: 464.2173156738281\n",
            "Bathc: 48/58, loss: 475.19537353515625\n",
            "Bathc: 49/58, loss: 468.20703125\n",
            "Bathc: 50/58, loss: 479.9344787597656\n",
            "Bathc: 51/58, loss: 444.8860778808594\n",
            "Bathc: 52/58, loss: 473.5705871582031\n",
            "Bathc: 53/58, loss: 427.7045593261719\n",
            "Bathc: 54/58, loss: 476.5636901855469\n",
            "Bathc: 55/58, loss: 473.4486999511719\n",
            "Bathc: 56/58, loss: 501.8087158203125\n",
            "Bathc: 57/58, loss: 467.6116638183594\n",
            "Bathc: 58/58, loss: 78.56304931640625\n",
            "val loss: 455.5892639160156\n",
            "Epoch: 6\n",
            "______________________\n",
            "Bathc: 1/58, loss: 451.5465393066406\n",
            "Bathc: 2/58, loss: 441.0413818359375\n",
            "Bathc: 3/58, loss: 420.4232177734375\n",
            "Bathc: 4/58, loss: 452.7574768066406\n",
            "Bathc: 5/58, loss: 420.2973937988281\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-26d67b781b3d>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {ep+1}\\n______________________\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0mtrainIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0mvalLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateValLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'val loss: {valLoss/len(testLoader)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-26d67b781b3d>\u001b[0m in \u001b[0;36mtrainIteration\u001b[0;34m(model, trainLoader, optimizer)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrainIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatchNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjCount\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjCount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/voc.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/vision.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2190\u001b[0m                 )\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def calculateLoss(classTensor, bboxTensor, bboxs, lbls, objCount):\n",
        "        loss = torch.tensor(0, device=device)\n",
        "        for iter in range(len(classTensor)):\n",
        "            classPr, bboxPr = classTensor[iter], bboxTensor[iter]\n",
        "            classPr, bboxPr = classPr.to(device), bboxPr.to(device)\n",
        "            # Получаем истинные значения ббоксов и меток классов для конкретного изображения\n",
        "            objCountAtImg = objCount[iter].item()\n",
        "            objCountBefore = objCount[:iter].sum().item()\n",
        "            bboxAtImg = bboxs[objCountBefore:objCountBefore+objCountAtImg]\n",
        "            lblAtImg = lbls[objCountBefore:objCountBefore+objCountAtImg]\n",
        "\n",
        "            # Здесь пройдут основные вычисления\n",
        "            with torch.no_grad():\n",
        "                iou = torchvision.ops.box_iou(torchvision.ops.box_convert(bboxPr*IMG_SIZE, 'cxcywh', 'xyxy'), torchvision.ops.box_convert(bboxAtImg*IMG_SIZE, 'cxcywh', 'xyxy'))\n",
        "                iouMax = iou.argmax(0)\n",
        "                iouBig = torch.where((iou> 0.5).sum(1) >= 1)[0]\n",
        "                iouBig = iouBig[~torch.isin(iouBig, iouMax)]\n",
        "                iouAll = torch.cat([iouMax, iouBig])\n",
        "                mask = torch.ones(len(iou)).bool()\n",
        "                mask[iouAll] = False\n",
        "                negClassTarget = torch.tensor([CLASS_COUNT - 1] * (classPr[mask].shape[0])).to(device)\n",
        "\n",
        "\n",
        "            loss = loss + F.cross_entropy(classPr[iouMax], lblAtImg) # Добавляем ошибку максимальных IoU по классификации\n",
        "            loss = loss + F.mse_loss(bboxPr[iouMax], bboxAtImg) # Добавляем ошибку максимальных IoU по расстоянию\n",
        "            loss = loss + F.cross_entropy(classPr[mask], negClassTarget)# Добавляем ошибку по негативному классу\n",
        "            if len(iouBig) != 0:\n",
        "                iouBigAM = iou[iouBig].argmax(1)\n",
        "                loss = loss + F.cross_entropy(classPr[iouBig], lblAtImg[iouBigAM]) # Добавляем ошибку IoU >=0.5 по классу\n",
        "                loss = loss + F.mse_loss(bboxPr[iouBig], bboxAtImg[iouBigAM]) # Добавляем ошибку IoU >=0.5 по расстоянию\n",
        "        return loss\n",
        "\n",
        "@torch.no_grad\n",
        "def calculateValLoss(model, testLoader):\n",
        "    model.eval()\n",
        "    loss = torch.tensor(0).to(device)\n",
        "    for batchNum, (imgs, bboxs, lbls, objCount) in enumerate(testLoader):\n",
        "        imgs, bboxs, lbls, objCount = imgs.to(device), bboxs.to(device), lbls.to(device), objCount\n",
        "        classTensor, bboxTensor = model(imgs)\n",
        "        classTensor, bboxTensor = convertModelResult(classTensor, bboxTensor)\n",
        "        loss = loss + calculateLoss(classTensor, bboxTensor, bboxs, lbls, objCount)\n",
        "    return loss\n",
        "\n",
        "def trainIteration(model, trainLoader, optimizer):\n",
        "    model.train()\n",
        "    for batchNum, (imgs, bboxs, lbls, objCount) in enumerate(trainLoader):\n",
        "        model.train()\n",
        "        imgs, bboxs, lbls, objCount = imgs.to(device), bboxs.to(device), lbls.to(device), objCount\n",
        "        classTensor, bboxTensor = model(imgs)\n",
        "        classTensor, bboxTensor = convertModelResult(classTensor, bboxTensor)\n",
        "        loss = calculateLoss(classTensor, bboxTensor, bboxs, lbls, objCount)\n",
        "        print(f'Bathc: {batchNum+1}/{len(trainLoader)}, loss: {loss}')\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        torch.save(model.state_dict(),'model.pth')\n",
        "\n",
        "\n",
        "for ep in range(EPOCH_COUNT):\n",
        "  print(f\"Epoch: {ep+1}\\n______________________\")\n",
        "  trainIteration(model, trainLoader, optimizer)\n",
        "  valLoss = calculateValLoss(model, testLoader)\n",
        "  print(f'val loss: {valLoss/len(testLoader)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iou shape - 7008 x 2\n",
        "#"
      ],
      "metadata": {
        "id": "KUOEmT8SUMH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}